---
title: "Temperature in Singapore"
execute: 
  eval: true
  echo: true
  freeze: true
  warning: false
format:
  html:
    code-fold: true
    code-summary: "Show the code"
editor: visual
---

# **1 Overview**

In the research project "Be Weatherwise or Otherwise," we aim to visualize the weather conditions of the past decade, from 2014 to 2023, with clarity. This will assist future residents in making informed decisions when purchasing homes. Despite Singapore's relatively small geographic size, it experiences varied weather patterns across different regions.

For this take-home exercise, I will be responsible for the temperature part and accomplish the following tasks:

-   To evaluate and determine the necessary R packages needed for your Shiny application are supported in R CRAN,

-   To prepare and test the specific R codes can be run and returned the correct output as expected,

-   To determine the parameters and outputs that will be exposed on the Shiny applications, and

-   To select the appropriate Shiny UI components for exposing the parameters determine above.

I will utilize suitable R packages for data visualization. Additionally, I will provide a detailed discussion and explanation of:

-   the data preparation process,

-   the selection of data visualisation techniques used,

-   and the data visualisation design and interactivity principles and best practices implemented.

# **2 Data Preparation**

## **2.1 Loading R Packages**

In this take-home exercise, the key R package use is [tmap](https://cran.r-project.org/web/packages/tmap/) package in R. Beside tmap package, four other R packages will be used. They are:

-   [**readr**](https://readr.tidyverse.org/) for importing delimited text file,

-   [**tidyr**](https://tidyr.tidyverse.org/) for tidying data

-   [**dplyr**](https://dplyr.tidyverse.org/) for wrangling data

-   [**sf**](https://cran.r-project.org/web/packages/sf/) for handling geospatial data.

-   [**tmap**](https://cran.r-project.org/web/packages/tmap/) for Creating thematic maps.

Among the four packages, readr, tidyr and dplyr are part of tidyverse package.

The cod,sf,spe chunk below will be used to install and load these packages in RStudio.

```{r}
#| code-fold: false
pacman::p_load(ggplot2,readr,readxl, dplyr,lubridate,reshape2,pheatmap, tidyverse,sf,sp,viridis,terra,gstat,tibble,leaflet,tmap)

```

## **2.2 Importing Data**

We obtained Singapore weather data for the years 2014 to 2023 from the [Meteorological Service Singapore](http://www.weather.gov.sg/climate-historical-daily/)website, along with latitude and longitude data records for different sites. We will proceed to import this data.

```{r}
#| code-fold: false
data <- read.csv("data/weather.csv")
station_records <- read_excel("data/station_records.xlsx")
```

## **2.3 Summary Statistics of Data**

First, let's take a look at the basic structure of the data.

```{r}
#| code-fold: false
summary(data)
```

```{r}
#| code-fold: false
glimpse(data)
```

From the basic overview of the data, we can see that there are 204,464 rows and 13 columns.

## **2.4 Cleanse missing values**

Additionally, there is a significant presence of missing values indicated by the "?" and "�" characters within the data. This suggests that data cleaning will be necessary as part of our subsequent analysis.

```{r}
#| code-fold: false
data <- read_csv("data/weather.csv", na = c("?", "�"))

data <- data %>%
  dplyr::filter(Year >= 2014, Year <= 2023)

colnames(data) <- c(
  'Station', 'Year', 'Month', 'Day', 'DailyRainfall',
  'Highest30minRainfall', 'Highest60minRainfall', 'Highest120minRainfall',
  'MeanTemperature', 'MaxTemperature', 'MinTemperature',
  'MeanWindSpeed', 'MaxWindSpeed'
)

data <- data %>%
  mutate(
    DailyRainfall = as.numeric(DailyRainfall),
    Highest30minRainfall = as.numeric(Highest30minRainfall),
    Highest60minRainfall = as.numeric(Highest60minRainfall),
    Highest120minRainfall = as.numeric(Highest120minRainfall),
    MeanTemperature = as.numeric(MeanTemperature),
    MaxTemperature = as.numeric(MaxTemperature),
    MinTemperature = as.numeric(MinTemperature)
  ) %>%
  
  suppressWarnings()
```

## **2.5 Handling missing values**

Due to the excessive number of missing values in temperature data, I will adopt a certain strategy. If MeanTemperature is available, it will be retained. However, if it is missing, I will use the average of MaxTemperature and MinTemperature as the daily mean temperature. All temperatures will then be recorded as CalculatedMeanTemp.

Additionally, I will compute the monthly average data (monthly_avg_temp) for future using.

```{r}
# Prepare the data: calculate mean temperature if it's not provided
data <- data %>%
  mutate(CalculatedMeanTemp = ifelse(is.na(MeanTemperature),
                                     (MaxTemperature + MinTemperature) / 2,
                                     MeanTemperature))

# Calculate monthly average temperature for each station
monthly_avg_temp <- data %>%
  group_by(Station, Year, Month) %>%
  summarise(MonthlyAvgTemp = mean(CalculatedMeanTemp, na.rm = TRUE), .groups = 'drop')

monthly_avg_temp
```

After implementing the above strategy, I will recalculate the total number of missing values for each site.

```{r}
# Calculate the total number of missing values per station
missing_values_by_station <- monthly_avg_temp %>%
  group_by(Station) %>%
  summarise(TotalMissing = sum(is.na(MonthlyAvgTemp), na.rm = TRUE)) %>%
  arrange(desc(TotalMissing))

missing_values_by_station
```

I will record stations with more than 30 missing values as "stations_with_high_missing".

```{r}
stations_with_high_missing <- missing_values_by_station %>%
  filter(TotalMissing > 30)

stations_with_high_missing
```

I will remove stations with TotalMissing greater than 30 from the monthly average data, as they will no longer be included in our project going forward.

```{r}
# On a monthly basis
# Remove stations with TotalMissing greater than 30 from the monthly average data
monthly_data_filtered <- monthly_avg_temp %>%
  anti_join(stations_with_high_missing, by = "Station")

monthly_data_filtered 
```

I will also remove stations with TotalMissing greater than 30 from the original data (data) and save it as data_with_mean_temp for future reference.

```{r}
# By the day
# Remove stations with TotalMissing greater than 30 from the raw data
data_filtered <- data %>%
  anti_join(stations_with_high_missing, by = "Station")

data_with_mean_temp <- data_filtered %>%
  mutate(CalculatedMeanTemp = ifelse(is.na(MeanTemperature),
                                  (MaxTemperature + MinTemperature) / 2,
                                  MeanTemperature))


data_with_mean_temp

```

## **2.6 Dataset Import**

In the end, we have obtained two usable datasets: one is the monthly average dataset 'monthly_data_filtered', and the other is the daily average dataset 'data_with_mean_temp'.

```{r}
write_rds(data_with_mean_temp,
          "data/data_with_mean_temp.rds")
data_with_mean_temp <- read_rds("data/data_with_mean_temp.rds")
write.csv(data_with_mean_temp, "data/data_with_mean_temp.csv")
```

```{r}
write_rds(monthly_data_filtered ,
          "data/monthly_data_filtered .rds")
monthly_data_filtered <- read_rds("data/monthly_data_filtered .rds")
write.csv(monthly_data_filtered, "data/monthly_data_filtered.csv")
```

# **3 Exploratory Data Analysis**

## **Heat Map**

Here, I have chosen to create a heatmap using the data for the 12 months of 2023, including data for each day.

```{r}

# Filter the data for 2023
data_2023 <- filter(data_with_mean_temp, Year == 2023)

# Calculate the average daily temperature
daily_mean_temp_2023 <- data_2023 %>%
  group_by(Month, Day) %>%
  summarise(CalculatedMeanTemp = mean(CalculatedMeanTemp, na.rm = TRUE)) %>%
  ungroup()

wide_data <- dcast(daily_mean_temp_2023, Day ~ Month, value.var = "CalculatedMeanTemp")

long_data <- melt(wide_data, id.vars = "Day", variable.name = "Month", value.name = "CalculatedMeanTemp")

ggplot(long_data, aes(x = Month, y = Day, fill = CalculatedMeanTemp))+
  geom_tile() +
  scale_fill_gradient(low = "white", high = "red", name = "Mean Temp (°C)") +
  labs(title = "2023 Daily Mean Temperature Heatmap in Singapore", x = "Month", y = "Day") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # 旋转X轴标签以便更好地显示

```

It can be observed that, on average, temperatures tend to be relatively low from December to March, while they peak from May to October, reaching the highest levels of the year.

```{r}
data$Date <- make_date(data$Year, data$Month, data$Day)
data$WeekOfMonth <- ceiling(day(data$Date) / 7)

pivot_data <- data %>%
  group_by(Year, Month, WeekOfMonth) %>%
  summarize(CalculatedMeanTemp = mean(CalculatedMeanTemp, na.rm = TRUE), .groups = 'drop') %>%
  pivot_wider(names_from = Year, values_from = CalculatedMeanTemp)

pivot_melted <- pivot_data %>%
  pivot_longer(cols = -c(Month, WeekOfMonth), names_to = "Year", values_to = "Temperature") %>%
  mutate(Month = factor(Month, levels = 1:12, labels = month.abb)) %>%
  arrange(Month, WeekOfMonth, Year)

ggplot(pivot_melted, aes(x = Year, y = interaction(Month, WeekOfMonth, sep = "-"), fill = Temperature)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = median(pivot_melted$Temperature, na.rm = TRUE)) +
  theme_minimal() +
  labs(x = "Year", y = "Month - Week of Month", fill = "Mean Temperature") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

From this heatmap, it can be observed that as the years progress, the temperature range gradually increases. This may also reflect an increasing likelihood of extreme weather events occurring.

# **4 Clustering Analysis**

## **4.1 Clustering by Station**

We will perform clustering analysis on the monthly data. To conduct the clustering analysis, I will first remove all missing values.

```{r}
clean_data <- na.omit(monthly_data_filtered)

clean_data
```

For the different temperatures in each region, as the temperature range differences are not very large, I will divide them into three categories and perform clustering to see which stations have similar average temperatures.

```{r}
station_means <- aggregate(MonthlyAvgTemp ~ Station, clean_data, mean)


set.seed(123) 
k <- 3

cluster_result <- kmeans(station_means$MonthlyAvgTemp, centers = k)

station_means$Cluster <- cluster_result$cluster

ggplot(station_means, aes(x = Station, y = MonthlyAvgTemp, color = factor(Cluster))) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

From the graph, we can see that blue points represent stations with higher average temperatures, green points represent stations with moderate average temperatures, and red points represent stations with lower average temperatures.

## **4.1 Clustering by Month**

In addition to clustering by site, we can also cluster by month. We will cluster into two groups to determine which months have lower temperatures and which months have higher temperatures.

```{r}
monthly_avg_temp <- clean_data %>%
  group_by(Month) %>%
  summarize(AvgTemp = mean(MonthlyAvgTemp))

set.seed(42) 
kmeans_result <- kmeans(monthly_avg_temp$AvgTemp, centers = 2)

monthly_avg_temp$Cluster <- as.factor(kmeans_result$cluster)

ggplot(monthly_avg_temp, aes(x = Month, y = AvgTemp, color = Cluster)) +
  geom_point(size = 4) +
  geom_line(aes(group = 1)) + 
  scale_color_manual(values = c("lightblue", "salmon")) +
  labs(title = "Cluster Analysis of Months Based on Average Temperature",
       x = "Month", y = "Average Temperature", color = "Cluster") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_continuous(breaks = 1:12) 
```

# **4 Data Visualization**

To create an interactive map, we need to merge the monthly average temperature data with the geographical location information of the stations.

## **4.1 Interactive Map (with Site Selection for Details Viewing)**

```{r}
merged_data <- left_join(monthly_data_filtered , station_records, by = "Station")

merged_data 

```

```{r}
#geometry
merdata_sf <- st_as_sf(merged_data ,
                      coords = c("Longitude",
                                 "Latitude"),
                      crs = 4326) %>%
  st_transform(crs=3414)
```

```{r}
mpsz2019 <-st_read(dsn = "data/geospatial",layer ="MPSZ-2019") %>%
  st_transform(CRS =3414)
```

```{r}
valid_geom <- st_is_valid(mpsz2019)

if (any(!valid_geom)) {
  cat("Invalid geometries detected:", which(!valid_geom), "\n")
  
  mpsz2019 <- st_make_valid(mpsz2019)
}

```

I will add hover information on the station basis, including Station, Year, Month, and Average Temperature (°C).

```{r}
library(gstat)
library(sf)
library(sp)
library(raster)

jan_2023_data <- merged_data[merged_data$Year == 2023 & merged_data$Month == 1, ]

jan_2023_sf <- st_as_sf(jan_2023_data, coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant")

pal <- colorNumeric(palette = "YlOrRd", domain = jan_2023_sf$MonthlyAvgTemp)

tmap_mode("view")

tm <- tm_shape(mpsz2019) +
  tm_borders() +
  tm_shape(jan_2023_sf) +
  tm_dots(col = "MonthlyAvgTemp", palette = "YlOrRd", size = 0.2, 
          popup.vars = c("Station" = "Station", "Year" = "Year", "Month" = "Month", "Avg Temp(°C)" = "MonthlyAvgTemp")) +
  tm_layout(title = "2023 January Average Temperature in Singapore")

tm
```

## **4.2 Map of Average Temperature Distribution in 2023**

Calculate the average temperature for each station in the year 2023 (ignoring missing values). Then, remove any records where AvgTp is NA due to missing values. Finally, display this processed dataset.

```{r}
tpData1 <- data_2023 %>%
  group_by(Station) %>%
  summarise(AvgTp = mean(MeanTemperature, na.rm = TRUE)) %>%
  filter(!is.na(AvgTp)) %>%
  ungroup() 
print(tpData1)
```

Firstly, merge the 'tpData1' and 'station_records' datasets using a left join. Then, convert the merged data into a spatial data format. Finally, transform its coordinate system for further geographical spatial analysis.

```{r}
tpData1 <- tpData1 %>%
  left_join(station_records)

tpData_sf1 <- st_as_sf(tpData1,
                      coords = c("Longitude",
                                 "Latitude"),
                      crs = 4326) %>%
  st_transform(crs=3414)
```

Continuing with the geographic spatial data processing, this involves:

-   Creating raster data and performing coordinate transformations,

-   Establishing geographic spatial point data, and

-   Filtering these point data based on specific spatial extents.

```{r}
grid <- terra::rast(mpsz2019, nrows = 690, ncols = 1075)
xy <- terra::xyFromCell(grid, 1:ncell(grid))

sf::sf_use_s2(FALSE)

coop <- st_as_sf(as.data.frame(xy),
                 coords = c("x","y"),
                 crs = st_crs(mpsz2019))

coop <- st_filter(coop,mpsz2019)
```

I set up a geostatistical model for spatial interpolation and confirmed the coordinate reference system of the data to prepare for subsequent spatial analysis.

```{r}
res <- gstat(formula = AvgTp ~ 1,
             locations = tpData_sf1,
             nmax = 15,
             set = list(idp = 0))

tpData_sf_crs1 <- st_crs(tpData_sf1)

print(tpData_sf_crs1)

```

Ensure that the spatial points "coop" used for prediction and the analysis model "tpData_sf1" are in the same coordinate system. Then, utilize kriging interpolation or similar spatial interpolation methods to predict the average temperature for these points. Use 'st_transform' for coordinate system transformation, and 'predict' for predicting temperature values after transforming the coordinate system.

```{r}
coop <- st_transform(coop, crs = tpData_sf_crs1)
resp <- predict(res,coop)
```

\
First, spatial data was prepared through coordinate transformation and rasterization. Then, the tmap package was utilized to draw a raster map, displaying the predicted temperature values across space.

```{r}
resp <- st_transform(resp, crs = terra::crs(grid))

resp$x <- st_coordinates(resp)[,1]
resp$y <- st_coordinates(resp)[,2]
resp$pred <- resp$var1.pred

pred <- terra::rasterize(resp, grid, field = "pred", fun = 'mean')

#print(terra::values(pred))

tmap_options(check.and.fix = TRUE)
tmap_mode("view")
tm_shape(pred) +
  tm_raster(alpha = 0.6, palette = "-viridis", n = 6) 
```

From the graph, we can observe:

The color legend indicates a temperature range from approximately 27.92 to 28.04 degrees Celsius.

Warm colors (yellow to green) represent areas with higher temperatures.

Cool colors (blue to purple) represent areas with lower temperatures.

-   Northern Region: The northern region appears at the top of the graph, indicating slightly lower temperatures according to the legend.

-   Central (Southern Location): The southern region appears at the bottom of the graph, displaying blue to purple hues, suggesting temperatures in the south are relatively moderate, warmer than the northern region.

-   Eastern Region: The eastern region is situated on the right side of the graph, with average temperatures falling within a moderate range.

-   Western Region: The western region is located on the left side of the graph, with temperatures leaning towards the higher side. The northwest tends to be slightly cooler, while temperatures in the southwest may be higher.

# 
